---
title: "Forecasting US Natural Gas Consumption"
author: "Marc Pastor"
date: "March 10th 2020"
output: html_document
---
<style>
pre {
  font-size: 14px;
}
</style>

## US Natural Gas Consumption 
In this project I am going to forecast the US Natural Gas Consumption for the following 12 months. The data used for this forecast  represents the monthly consumption of gas in the US between 2000 and 2019 (at the moment of this study the latest data is from July 2019). This is a dataset included in the R programming enviroment. 

## Loading the necessary libraries
```{r, results='hide', message=FALSE}
library(forecast)
library(TSstudio)
library(plotly)
library(tidyverse)
library(TSstudio)
library(plotly)
library(stats)
library(forecast)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(dygraphs)
library(lubridate)
library(datasets)
library(base)
library(h2o)
library(Quandl)
library(ggfortify)
```

## Loading the data into R
```{r, comment = ""}
data(USgas)
head(USgas, 40)
```

## Exploratory data analysis 
```{r}
ts_info(USgas)
```

***
```{r, fig.align="center", fig.height = 5, fig.width = 10}
autoplot(USgas) + ggtitle("Us Monthly Natural Gas Consumption") + 
        xlab("Year") + ylab("Billion Cubic Feet")
```

***
```{r, fig.align="center", fig.height = 5, fig.width = 10}
decompose(USgas) %>% plot()
```

## Exploratory analysis conclusions
We can see that the series has an additive structure, has trend which seems fairly linear and a very notorious seasonal component.

## Seasonality analysis
We first **convert** the time series into a data frame, because we will later use ggplots. We also create a year, month and day columns (this will come in handy during the analysis).
```{r}
USgas_df <- ts_to_prophet(USgas)
USgas_df$year <- lubridate::year(USgas_df$ds)
USgas_df$month <- lubridate::month(USgas_df$ds)
USgas_df$day <- lubridate::day(USgas_df$ds)
USgas_df <- USgas_df[,c(1,3,4,5,2)]
names(USgas_df) <- c("date", "year", "month", "day", "USgas")
```
***
We can see there's a seasonal component, because the density plots aren't overlapping 
```{r, fig.align = "center", fig.height = 4, fig.width = 8, size="small"}
ggplot(data = USgas_df, aes(x = USgas)) + geom_density(data = USgas_df, aes(fill = month)) + 
        ggtitle("USgas - Density plots by Month") + facet_grid(month ~.)
```

***
To analyse the real seasonal pattern, we detrend the series and plot the density plots for each month.
```{r, message=FALSE, warning = FALSE, fig.align="center"}
USgas_df$USgas_detrend <- USgas_df$USgas - decompose(USgas)$trend
ggplot(data = USgas_df, aes(x = USgas_detrend)) + geom_density(data = USgas_df, aes(fill = month)) +
        ggtitle("USgas - Density plots by Month (detrended series)") + facet_grid(month ~.)
```

***
```{r, fig.align="center", fig.height=5, fig.width=10}
ts_seasonal(USgas, type = "all")
```

## Seasonality conclusions
- The data has got a very important monthly seasonal component. We can see that the USgas consumption is higher in the winter months (December, January and February) and it reduces in spring, summer and autumn.  
- There has been an increasing trend over the past years.


## Correlation analysis
```{r, fig.align="center", fig.height=4, fig.width=10}
par(mfrow = c(1,2))
acf(USgas, lag.max = 60)
pacf(USgas, lag.max = 60)
```

***
```{r, fig.align="center", fig.height=5, fig.width=10}
ts_lags(USgas)
```

## Correlation conclusions
- In the acf we have observed  a clearly linear decay (this means that if we use an ARIMA model we will have to differentiate, because linear decay is an indicator of trend in the series, and so it won't be a stationary series)
- Also we see the series has a strong correlation with its first non-seasonal lag as well with its first seasonal lag (lag 12)

## Creating the training and testing partitions
```{r}
USgas_split <- ts_split(USgas, sample.out = 12)
train <- USgas_split$train
test <- USgas_split$test
```

# Forecasting approaches

# Multivariate Linear Regression

## 1. Linear Regression (trend & season)
```{r, message = FALSE, comment = "", fig.height=5, fig.width=10}
tslm_1 <- tslm(train ~ season + trend)
summary(tslm_1)
```

***
Since all the variables are statistically significant we proceed to train the model.
```{r, message = FALSE, comment = ""}
fc_tslm1 <- forecast(tslm_1, h = 12)
accuracy(fc_tslm1, test)
```
The model seems to be overfitting since it obtains a higher result in all the error metrics in the testing partition compared to the training set.

*** 
```{r, fig.align="center", fig.height=4.3, fig.width=10}
test_forecast(actual = USgas,
              forecast.obj = fc_tslm1,
              test = test)
```

***
```{r, message = FALSE, comment = "", fig.align="center", fig.height=4.3, fig.width=8}
checkresiduals(tslm_1)
```

## Conclusions about first linear regression model (modeling with trend and season components)
- All the variables are statistically significant (they're not independent).
- Residuals are not white noise (the model didn't capture all the patterns).
- Residuals are correlated with their lags and they are fairly normally distributed.

## 2. Linear Regression (trend, season & seasonal lag)
First we have to transform the time series into a dataframe, in order to create a new variable (lag 12), which represents the first seasonal lag of the series. 
```{r}
USgas_df_2 <- USgas_df %>% dplyr::select(date, USgas)
USgas_df_2$lag12 <- dplyr::lag(USgas_df_2$USgas, 12)
USgas_df_2 <- filter(USgas_df_2, !is.na(lag12))
```
Then we create the training and testing partitions.
```{r}
train_df_2 <- USgas_df_2[1:(nrow(USgas_df_2) - 12),]
test_df_2 <- USgas_df_2[(nrow(USgas_df_2) - 12 + 1):nrow(USgas_df_2),]
```
Finally we convert the new dataframe into a time series (after removing the NA's from the new lag12 column)
```{r}
USgas_2 <- ts(USgas_df_2[,2], start = c(lubridate::year(min(USgas_df_2$date)), lubridate::month(min(USgas_df_2$date))), frequency = 12)
USgas_2_split <- ts_split(USgas_2, sample.out = 12)
train_2 <- USgas_2_split$train
test_2 <- USgas_2_split$test
```

***
Now we train the model. We see that all the variables are statistically significant.
```{r, message = FALSE, comment = "", fig.align="center"}
tslm_2 <- tslm(train_2 ~ season + trend + lag12, data = train_df_2)
summary(tslm_2)
```

## Analyzing the model 
First we use the training set to forecast the testing values, and compare the fitted values with the real values.
```{r}
fc_tslm2 <- forecast(tslm_2, h = 12, newdata = test_df_2)
accuracy(fc_tslm2, test_2)
```
We can see that there has been an improvement in the model's behaviour, since the error metrics from the training and testing partitions are pretty close (this indicates that the model is probably not overfitting). 

***
In this slide we visualize the fitted values by the model and the real values from the testing partition.
```{r, fig.align="center", fig.height=4, fig.width=10}
test_forecast(actual = USgas_2,
              forecast.obj = fc_tslm2,
              test = test_2)
```

***
```{r, fig.align="center", fig.height=4.3, fig.width=8}
checkresiduals(fc_tslm2)
```

## Conclusions
- The model seems to be capturing better the patterns of the data, since most of the residuals aren't correlated with their lags. But on the other hand, since there are some residuals correlated with their lags the model is not capturing all the patterns in the data.
- Residuals seem to be fairly normally distributed.
- All in all, this model seems to be performing better than the first one, but it isn't perfect. A better approach is to use SARIMA models.

# ARIMA and SARIMA Models

***
**Lag Analysis** : We can see there's a linear decay in the acf plot, which indicates that the series is not stationary (this is caused bc of the effect of the trend).
Also in the pacf we see that the series is very correlated with its first non-seasonal and seasonal lags.
```{r, fig.align="center", fig.width=8}
par(mfrow = c(1,2))
acf(USgas, lag.max = 60)
pacf(USgas, lag.max = 60)
```

***
**Lag Analysis (First order non-seasonal difference)** : There still is a lot of linear decay and correlation with its lags in the pacf. Also, there still is a linear decay in the acf, indicating that there still is a trend (and therefore the series is not stationary).
```{r, fig.align="center", fig.height = 3.4, fig.width=8}
USgas_diff_1 <- diff(USgas, 1)
par(mfrow = c(1,2))
acf(USgas_diff_1, lag.max = 60) 
pacf(USgas_diff_1, lag.max = 60)
```

***
**Lag Analysis (First order non-seasonal difference)**: The first order difference stabilizes the series in the mean, but the variance is not very stable, so we will have to differenciate again.
```{r, fig.align="center", fig.width=8}
autoplot(USgas_diff_1)+ ggtitle("US National Gas Consumption - First order non-seasonal difference") + theme(plot.title = element_text(hjust = 0.5))
```

***
**Lag Analysis (First order non-seasonal difference and first order seasonal difference)** : Since the series is very correlated with its seasonal lags, we differenciate with respect to the first seasonal lag (lag 12, since its a monthly series). We can see how the linear decay has been removed, and how the correlations tail off in both acf in pacf. This is an indicator that the series will follow an **ARMA** structure.

```{r, fig.align="center", fig.height = 3.5, fig.width=8}
USgas_diff_1_12 <- diff(USgas_diff_1, 12)
par(mfrow = c(1,2))
acf(USgas_diff_1_12, lag.max = 60) 
pacf(USgas_diff_1_12, lag.max = 60)
```

***
**Lag Analysis (First order non-seasonal difference and first order seasonal difference)**: We can see how the series has stabilized its mean and variance.
```{r, fig.align="center", fig.width=8}
autoplot(USgas_diff_1_12)+ ggtitle("US National Gas Consumption - First order non-seasonal difference 
        and first order seasonal difference") + theme(plot.title = element_text(hjust = 0.5))
```

***
**Tuning the parameters (I)**: As we saw previously the series will follow an ARMA structure. We know the parameters d=1 and D=1 (since we differenciated with respect to the first seasonal and non-seasonal lags). We create a table with the possible combinations of the parameters, and k (the result of adding the parameters) and later we will create a function that creates SARIMA models with the different combinations of parameters, and returns the best models (rated by their AIC). 
```{r, fig.align="center", fig.width=8}
p <- q <- P <- Q <- 0:2
d <- D <- 1
arima_grid <- expand.grid(p,d,q,P,D,Q)
names(arima_grid) <- c("p","d","q","P","D", "Q")
arima_grid$k <- rowSums(arima_grid)
arima_grid <- filter(arima_grid, k <= 6)
head(arima_grid, 3)
```

***
**Tuning the parameters (II)**: We create the arima_search2 function which trains in the training partition SARIMA models with different parameters combinations and returns us the model's parameters and its respective AIC score (we filter those models with k <= 6). Then we select the 3 best models.
```{r, fig.align="center", fig.width=8, warning = FALSE}
arima_search2 <- function(x){
        for(i in 1:nrow(x)){
                md <- NULL
                md <- arima(train, order = c(x$p[i], 1, x$q[i]), 
                            seasonal = list(order = c(x$P[i], 1, x$Q[i])))
                x$AIC[i] <- md$aic
                x <- x %>% arrange(AIC)
        }
        x
}
best_arima_models <- arima_search2(arima_grid)
best_arima_models <- best_arima_models[1:3,]
best_arima_models
```

The best models are (from best to worst) **SARIMA(1,1,1)(0,1,2)**, **SARIMA(1,1,1)(0,1,1)**, **SARIMA(1,1,1)(1,1,1)**

# Analyzing SARIMA models performance

# SARIMA(1,1,1)(0,1,2)

***
**Training the model (I)**: 
```{r, fig.align="center", fig.width=8, warning = FALSE}
md_arima1 <- arima(train, order = c(1,1,1), seasonal = list(order = c(0,1,2)))
fc_arima1 <- forecast(md_arima1, h = 12)
accuracy(fc_arima1, test)
```
It has a similar MAPE in the training and testing partitions, so it doesn't seem to be overfitting

***
**Comparing fitted values and real values**
```{r, fig.align="center", fig.width=8, warning = FALSE}
test_forecast(actual = USgas,
              forecast.obj = fc_arima1,
              test = test)
```

***
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
checkresiduals(fc_arima1) 
```


## Model conclusion
- It seems to be capturing pretty well the patterns of the data
- Residuals and their lags seem to be pretty independent. Residuals are fairly normally distributed and white noise

# SARIMA(1,1,1)(0,1,1)

***
**Training the model (I)**: 
```{r, fig.align="center", fig.width=8, warning = FALSE}
md_arima2 <- arima(train, order = c(1,1,1), seasonal = list(order = c(0,1,1)))
fc_arima2 <- forecast(md_arima2, h = 12)
accuracy(fc_arima2, test)
```
It has a similar MAPE in the training and testing partitions, so it doesn't seem to be overfitting.

***
**Comparing fitted values and real values**
```{r, fig.align="center", fig.width=8, warning = FALSE}
test_forecast(actual = USgas, 
              forecast.obj = fc_arima2,
              test = test)
```

***
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
checkresiduals(fc_arima2) 
```

## Model conclusion
- Higher error rate than the first model (but not overfitting)
- Residuals and their lags seem to be pretty independent. Residuals are fairly normally distributed and white noise. 
- Similar residuals compared to the first model.

# SARIMA(1,1,1)(1,1,1)

***
**Training the model (I)**: 
```{r, fig.align="center", fig.width=8, warning = FALSE}
md_arima3 <- arima(train, order = c(1,1,1), seasonal = list(order = c(1,1,1)))
fc_arima3 <- forecast(md_arima3, h = 12)
accuracy(fc_arima3, test)
```
It has a similar MAPE in the training and testing partitions, so it doesn't seem to be overfitting.

***
**Comparing fitted values and real values**
```{r, fig.align="center", fig.width=8, warning = FALSE}
test_forecast(actual = USgas, 
              forecast.obj = fc_arima3,
              test = test)
```

***
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
checkresiduals(fc_arima3) 
```

## Model conclusion
- Higher error rate than the first two models (but not overfitting)
- Residuals and their lags seem to be pretty independent. Residuals are fairly normally distributed and white noise. 
- Similar residuals compared to the first model.

# auto.arima() model

***
**Training the model (I)**: 
```{r, fig.align="center", fig.width=8, warning = FALSE}
md_arima_auto <- auto.arima(train) 
fc_arima_auto <- forecast(md_arima_auto, h = 12)
accuracy(fc_arima_auto, test)
```
It has a similar MAPE in the training and testing partitions, so it doesn't seem to be overfitting.

***
**Comparing fitted values and real values**
```{r, fig.align="center", fig.width=8, warning = FALSE}
test_forecast(actual = USgas, 
              forecast.obj = fc_arima_auto,
              test = test)
```

***
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
checkresiduals(fc_arima_auto) 
```

## Model conclusion
- Higher AIC than the first three models
- Higher error rate than the first three models (but not overfitting)
- Residuals and their lags are pretty independent. Residuals are normally distributed and white noise. 
- Better residuals than the other models.

## Final decision
Finally after analyzing all the models, the best one is clearly the first one: **ARIMA(1,1,1)(0,1,2)**, so this is the model we are going to use to forecast the data.

# Forecast
## Forecasting (I)
```{r, warning=FALSE}
md_arima_final <- arima(USgas, order = c(1,1,1), seasonal = list(order = c(0,1,2)))
fc_arima_final <- forecast(md_arima_final, h = 12)
```

## Forecasting next year (II) 
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
autoplot(fc_arima_final) + 
        ggtitle("Forecast: US Gas Natural Gas Consumption using SARIMA(1,1,1)(0,1,2)") + 
        xlab("Year") + ylab("Billion Cubic Feet")
```

## Forecasting next year - Zoomed (III) 
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
autoplot(fc_arima_final) + 
        ggtitle("Forecast: US Gas Natural Gas Consumption using SARIMA(1,1,1)(0,1,2)") + 
        xlab("Year") + ylab("Billion Cubic Feet") + xlim(as.Date(c("2014-01-01", "2021-01-01")))
```

## Forecasting next 3 years (IV)
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
fc_arima_final_2 <- forecast(md_arima_final, h = 36)
autoplot(fc_arima_final_2) + ggtitle("Forecast: US Gas Natural Gas Consumption using SARIMA(1,1,1)(0,1,2)") + 
        xlab("Year") + ylab("Billion Cubic Feet")
```

## Forecasting next 3 years - Zoomed (V)
```{r, fig.align="center", fig.width=8, warning = FALSE, fig.height=4.3, fig.width=8}
autoplot(fc_arima_final_2) + ggtitle("Forecast: US Gas Natural Gas Consumption using SARIMA(1,1,1)(0,1,2)") + 
        xlab("Year") + ylab("Billion Cubic Feet") + xlim(as.Date(c("2014-01-01", "2022-11-01")))
```

# Thanks for your attention!